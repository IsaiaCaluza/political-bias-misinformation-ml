{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlkSxTTZ0B1U"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "class PoliticalBiasLogisticRegression:\n",
        "    \"\"\"\n",
        "    Logistic Regression model for political bias classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_iter=1000, random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize the logistic regression model.\n",
        "\n",
        "        Args:\n",
        "            max_iter (int): Maximum number of iterations for optimization\n",
        "            random_state (int): Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.model = LogisticRegression(max_iter=max_iter, random_state=random_state)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Train the logistic regression model.\n",
        "\n",
        "        Args:\n",
        "            X_train: Training features\n",
        "            y_train: Training labels\n",
        "\n",
        "        Returns:\n",
        "            self: The trained model instance\n",
        "        \"\"\"\n",
        "        self.model.fit(X_train, y_train)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions with the trained model.\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix to predict on\n",
        "\n",
        "        Returns:\n",
        "            array: Predicted class labels\n",
        "        \"\"\"\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict class probabilities for the input features.\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix to predict on\n",
        "\n",
        "        Returns:\n",
        "            array: Predicted class probabilities\n",
        "        \"\"\"\n",
        "        return self.model.predict_proba(X)\n",
        "\n",
        "    def evaluate(self, X_test, y_test, target_names=[\"Left\", \"Center\", \"Right\"]):\n",
        "        \"\"\"\n",
        "        Evaluate the model and print classification report.\n",
        "\n",
        "        Args:\n",
        "            X_test: Test features\n",
        "            y_test: True test labels\n",
        "            target_names (list): Names of target classes\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing evaluation metrics\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X_test)\n",
        "        report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
        "        print(f\"ðŸ“Š Logistic Regression Results:\")\n",
        "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "        return {\n",
        "            'y_pred': y_pred,\n",
        "            'probabilities': self.predict_proba(X_test),\n",
        "            'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
        "            'report': report,\n",
        "            'accuracy': report['accuracy'],\n",
        "            'f1': report['macro avg']['f1-score'],\n",
        "            'precision': report['macro avg']['precision'],\n",
        "            'recall': report['macro avg']['recall']\n",
        "        }\n",
        "\n",
        "    def get_feature_importance(self, feature_names=None, top_n=20):\n",
        "        \"\"\"\n",
        "        Get the most important features for each class.\n",
        "\n",
        "        Args:\n",
        "            feature_names (list): Names of the features (e.g., from vectorizer.get_feature_names_out())\n",
        "            top_n (int): Number of top features to return\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary with top features for each class\n",
        "        \"\"\"\n",
        "        if not hasattr(self.model, 'coef_'):\n",
        "            raise ValueError(\"Model doesn't have coefficients. Make sure it's trained.\")\n",
        "\n",
        "        if feature_names is None:\n",
        "            feature_names = [f\"feature_{i}\" for i in range(self.model.coef_.shape[1])]\n",
        "\n",
        "        # Get coefficients for each class\n",
        "        coef = self.model.coef_\n",
        "\n",
        "        # Map class indices to names\n",
        "        class_names = self.model.classes_.tolist()\n",
        "        if hasattr(self.model, 'target_names_'):\n",
        "            class_name_map = {i: self.model.target_names_[i] for i in range(len(self.model.target_names_))}\n",
        "        else:\n",
        "            class_name_map = {i: f\"Class {i}\" for i in range(len(class_names))}\n",
        "\n",
        "        # Create dictionary to hold top features for each class\n",
        "        top_features = {}\n",
        "\n",
        "        # For each class, get the top positive features\n",
        "        for i, class_coef in enumerate(coef):\n",
        "            class_name = class_name_map.get(class_names[i], f\"Class {class_names[i]}\")\n",
        "\n",
        "            # Sort features by coefficient value (descending)\n",
        "            sorted_indices = class_coef.argsort()[::-1]\n",
        "\n",
        "            # Get top positive and negative features\n",
        "            top_pos_indices = sorted_indices[:top_n]\n",
        "            top_neg_indices = sorted_indices[-top_n:]\n",
        "\n",
        "            # Map indices to feature names and coefficients\n",
        "            top_pos_features = [(feature_names[idx], class_coef[idx]) for idx in top_pos_indices]\n",
        "            top_neg_features = [(feature_names[idx], class_coef[idx]) for idx in top_neg_indices]\n",
        "\n",
        "            # Store in dictionary\n",
        "            top_features[class_name] = {\n",
        "                'positive': top_pos_features,\n",
        "                'negative': top_neg_features\n",
        "            }\n",
        "\n",
        "        return top_features\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"\n",
        "        Save the trained model to disk.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to save the model file\n",
        "\n",
        "        Returns:\n",
        "            str: Path to the saved model\n",
        "        \"\"\"\n",
        "        import pickle\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(self.model, f)\n",
        "        return filepath\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, filepath):\n",
        "        \"\"\"\n",
        "        Load a trained model from disk.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to the saved model file\n",
        "\n",
        "        Returns:\n",
        "            PoliticalBiasLogisticRegression: Loaded model instance\n",
        "        \"\"\"\n",
        "        import pickle\n",
        "        instance = cls()\n",
        "        with open(filepath, 'rb') as f:\n",
        "            instance.model = pickle.load(f)\n",
        "        return instance"
      ]
    }
  ]
}