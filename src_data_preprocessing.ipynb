{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3Pnbx6qzpHY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def get_custom_stopwords():\n",
        "    \"\"\"\n",
        "    Get custom stopwords list for political content analysis.\n",
        "\n",
        "    Returns:\n",
        "        list: Extended stopwords list with domain-specific terms\n",
        "    \"\"\"\n",
        "    return list(\n",
        "        set(TfidfVectorizer(stop_words='english').get_stop_words()).union({\n",
        "            'said', 'says', 'mr', 'mrs', 'would', 'also', 'one', 'u', 'us'\n",
        "        })\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess_text_data(df, content_column='content', label_column='bias', max_features=5000):\n",
        "    \"\"\"\n",
        "    Preprocess text data with TF-IDF vectorization and custom stopwords.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): Input dataframe with text and labels\n",
        "        content_column (str): Column name containing text content\n",
        "        label_column (str): Column name containing labels\n",
        "        max_features (int): Maximum number of features for TF-IDF\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X, y, vectorizer) where X is the TF-IDF matrix, y is the label array,\n",
        "               and vectorizer is the fitted TfidfVectorizer\n",
        "    \"\"\"\n",
        "    custom_stop_words = get_custom_stopwords()\n",
        "    vectorizer = TfidfVectorizer(stop_words=custom_stop_words, max_features=max_features)\n",
        "    X = vectorizer.fit_transform(df[content_column])\n",
        "    y = df[label_column]\n",
        "    return X, y, vectorizer\n",
        "\n",
        "\n",
        "def apply_smote(X, y, random_state=42):\n",
        "    \"\"\"\n",
        "    Apply SMOTE to balance classes.\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix\n",
        "        y: Target labels\n",
        "        random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X_res, y_res) balanced dataset\n",
        "    \"\"\"\n",
        "    smote = SMOTE(random_state=random_state)\n",
        "    X_res, y_res = smote.fit_resample(X, y)\n",
        "    return X_res, y_res\n",
        "\n",
        "\n",
        "def prepare_train_test_data(X, y, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Split data into training and testing sets.\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix\n",
        "        y: Target labels\n",
        "        test_size (float): Proportion of data to use for testing\n",
        "        random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X_train, X_test, y_train, y_test) split datasets\n",
        "    \"\"\"\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "\n",
        "def load_and_prepare_data(dataset_name=\"siddharthmb/article-bias-prediction-random-splits\",\n",
        "                         split=\"train\", apply_balancing=True, max_features=5000):\n",
        "    \"\"\"\n",
        "    Load dataset, preprocess text, and optionally apply SMOTE balancing.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): HuggingFace dataset name\n",
        "        split (str): Dataset split to use ('train', 'valid', 'test')\n",
        "        apply_balancing (bool): Whether to apply SMOTE for class balancing\n",
        "        max_features (int): Maximum number of TF-IDF features\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X, y, vectorizer) processed data\n",
        "    \"\"\"\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    # Load dataset\n",
        "    ds = load_dataset(dataset_name)\n",
        "    df = pd.DataFrame(ds[split])\n",
        "\n",
        "    # Preprocess with TF-IDF\n",
        "    X, y, vectorizer = preprocess_text_data(df, max_features=max_features)\n",
        "\n",
        "    # Apply SMOTE if requested\n",
        "    if apply_balancing:\n",
        "        X, y = apply_smote(X, y)\n",
        "\n",
        "    return X, y, vectorizer\n",
        "\n",
        "\n",
        "def save_processed_data(X, y, vectorizer, output_dir=\"data/processed\", prefix=\"tfidf\"):\n",
        "    \"\"\"\n",
        "    Save processed data to disk.\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix\n",
        "        y: Target labels\n",
        "        vectorizer: Fitted TF-IDF vectorizer\n",
        "        output_dir (str): Directory to save files\n",
        "        prefix (str): Prefix for filenames\n",
        "\n",
        "    Returns:\n",
        "        dict: Paths to saved files\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import pickle\n",
        "    from scipy import sparse\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save features as sparse matrix\n",
        "    features_path = os.path.join(output_dir, f\"{prefix}_features.npz\")\n",
        "    sparse.save_npz(features_path, X)\n",
        "\n",
        "    # Save labels\n",
        "    labels_path = os.path.join(output_dir, f\"{prefix}_labels.npy\")\n",
        "    np.save(labels_path, y)\n",
        "\n",
        "    # Save vectorizer\n",
        "    vectorizer_path = os.path.join(output_dir, f\"{prefix}_vectorizer.pkl\")\n",
        "    with open(vectorizer_path, \"wb\") as f:\n",
        "        pickle.dump(vectorizer, f)\n",
        "\n",
        "    return {\n",
        "        \"features\": features_path,\n",
        "        \"labels\": labels_path,\n",
        "        \"vectorizer\": vectorizer_path\n",
        "    }\n",
        "\n",
        "\n",
        "def load_processed_data(input_dir=\"data/processed\", prefix=\"tfidf\"):\n",
        "    \"\"\"\n",
        "    Load processed data from disk.\n",
        "\n",
        "    Args:\n",
        "        input_dir (str): Directory containing saved files\n",
        "        prefix (str): Prefix for filenames\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X, y, vectorizer) loaded data\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import pickle\n",
        "    from scipy import sparse\n",
        "\n",
        "    # Load features\n",
        "    features_path = os.path.join(input_dir, f\"{prefix}_features.npz\")\n",
        "    X = sparse.load_npz(features_path)\n",
        "\n",
        "    # Load labels\n",
        "    labels_path = os.path.join(input_dir, f\"{prefix}_labels.npy\")\n",
        "    y = np.load(labels_path)\n",
        "\n",
        "    # Load vectorizer\n",
        "    vectorizer_path = os.path.join(input_dir, f\"{prefix}_vectorizer.pkl\")\n",
        "    with open(vectorizer_path, \"rb\") as f:\n",
        "        vectorizer = pickle.load(f)\n",
        "\n",
        "    return X, y, vectorizer\n",
        "\n",
        "\n",
        "def analyze_class_distribution(y, class_names=None):\n",
        "    \"\"\"\n",
        "    Analyze and visualize class distribution.\n",
        "\n",
        "    Args:\n",
        "        y: Target labels\n",
        "        class_names (list): Names of classes\n",
        "\n",
        "    Returns:\n",
        "        tuple: (class_counts, class_proportions, figure) distribution statistics and visualization\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Count class frequencies\n",
        "    unique_classes, counts = np.unique(y, return_counts=True)\n",
        "\n",
        "    # Calculate proportions\n",
        "    total = len(y)\n",
        "    proportions = counts / total\n",
        "\n",
        "    # Prepare class names\n",
        "    if class_names is None:\n",
        "        class_names = [f\"Class {i}\" for i in unique_classes]\n",
        "\n",
        "    # Create dictionary of counts and proportions\n",
        "    class_counts = dict(zip(class_names, counts))\n",
        "    class_proportions = dict(zip(class_names, proportions))\n",
        "\n",
        "    # Create a bar plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bars = ax.bar(class_names, counts, color=['#3a86ff', '#8338ec', '#ff006e'])\n",
        "\n",
        "    # Add count and percentage labels\n",
        "    for i, (bar, count, prop) in enumerate(zip(bars, counts, proportions)):\n",
        "        height = bar.get_height()\n",
        "        ax.text(\n",
        "            bar.get_x() + bar.get_width()/2.,\n",
        "            height + 0.1,\n",
        "            f\"{count} ({prop:.1%})\",\n",
        "            ha='center', va='bottom'\n",
        "        )\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_xlabel('Class')\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title('Class Distribution')\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    return class_counts, class_proportions, fig"
      ]
    }
  ]
}