{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3pxkNQk0VWD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DistilBertForSequenceClassification\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "class WeightedLossModel(DistilBertForSequenceClassification):\n",
        "    \"\"\"\n",
        "    Custom DistilBERT model with weighted loss for handling class imbalance.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained_model_name_or_path, num_labels, class_weights):\n",
        "        \"\"\"\n",
        "        Initialize the weighted loss model.\n",
        "\n",
        "        Args:\n",
        "            pretrained_model_name_or_path (str): Pretrained model name or path\n",
        "            num_labels (int): Number of output labels\n",
        "            class_weights (tensor): Class weights for loss function\n",
        "        \"\"\"\n",
        "        super().__init__.from_pretrained(pretrained_model_name_or_path, num_labels=num_labels)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        \"\"\"\n",
        "        Compute weighted loss during training.\n",
        "\n",
        "        Args:\n",
        "            model: The model being trained\n",
        "            inputs: Model inputs\n",
        "            return_outputs (bool): Whether to return model outputs\n",
        "\n",
        "        Returns:\n",
        "            tensor or tuple: Loss tensor or (loss, outputs) tuple\n",
        "        \"\"\"\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(\n",
        "            weight=self.class_weights.to(logits.device)\n",
        "        )\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "class PoliticalBiasTransformer:\n",
        "    \"\"\"\n",
        "    Transformer-based model for political bias classification using DistilBERT.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"distilbert-base-uncased\", num_labels=3, max_length=512):\n",
        "        \"\"\"\n",
        "        Initialize the transformer model.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Pretrained model name\n",
        "            num_labels (int): Number of output labels\n",
        "            max_length (int): Maximum sequence length for tokenizer\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.num_labels = num_labels\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        self.training_args = None\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        \"\"\"\n",
        "        Tokenize input texts for transformer model.\n",
        "\n",
        "        Args:\n",
        "            examples (dict): Dictionary containing text examples\n",
        "\n",
        "        Returns:\n",
        "            dict: Tokenized examples\n",
        "        \"\"\"\n",
        "        return self.tokenizer(\n",
        "            examples[\"content\"],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "    def encode_labels(self, example):\n",
        "        \"\"\"\n",
        "        Encode label field for the transformer model.\n",
        "\n",
        "        Args:\n",
        "            example (dict): Example dictionary\n",
        "\n",
        "        Returns:\n",
        "            dict: Example with encoded labels\n",
        "        \"\"\"\n",
        "        example[\"label\"] = example[\"bias\"]\n",
        "        return example\n",
        "\n",
        "    def prepare_dataset(self, dataset):\n",
        "        \"\"\"\n",
        "        Prepare a dataset for transformer model training.\n",
        "\n",
        "        Args:\n",
        "            dataset: Input dataset\n",
        "\n",
        "        Returns:\n",
        "            dataset: Processed dataset ready for training\n",
        "        \"\"\"\n",
        "        # Tokenize the text\n",
        "        tokenized_dataset = dataset.map(self.tokenize_function, batched=True)\n",
        "\n",
        "        # Encode labels\n",
        "        return tokenized_dataset.map(self.encode_labels)\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset=None, output_dir=\"./bert_results\",\n",
        "              batch_size=16, learning_rate=2e-5, num_epochs=3, weight_decay=0.01):\n",
        "        \"\"\"\n",
        "        Train the transformer model.\n",
        "\n",
        "        Args:\n",
        "            train_dataset: Training dataset\n",
        "            eval_dataset: Evaluation dataset\n",
        "            output_dir (str): Directory to save model outputs\n",
        "            batch_size (int): Training batch size\n",
        "            learning_rate (float): Learning rate\n",
        "            num_epochs (int): Number of training epochs\n",
        "            weight_decay (float): Weight decay for regularization\n",
        "\n",
        "        Returns:\n",
        "            self: Trained model instance\n",
        "        \"\"\"\n",
        "        # Prepare datasets\n",
        "        train_data = self.prepare_dataset(train_dataset)\n",
        "        eval_data = None if eval_dataset is None else self.prepare_dataset(eval_dataset)\n",
        "\n",
        "        # Compute class weights\n",
        "        labels = train_data['label']\n",
        "        class_weights = compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=np.unique(labels),\n",
        "            y=labels\n",
        "        )\n",
        "        weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "        # Initialize the model with class weights\n",
        "        self.model = WeightedLossModel(\n",
        "            pretrained_model_name_or_path=self.model_name,\n",
        "            num_labels=self.num_labels,\n",
        "            class_weights=weights_tensor\n",
        "        )\n",
        "\n",
        "        # Training arguments\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            num_train_epochs=num_epochs,\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "            logging_steps=100,\n",
        "            save_steps=1000,\n",
        "            eval_steps=500,\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        # Initialize trainer\n",
        "        self.trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=self.training_args,\n",
        "            train_dataset=train_data,\n",
        "            eval_dataset=eval_data,\n",
        "            tokenizer=self.tokenizer,\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        self.trainer.train()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def evaluate(self, test_dataset, target_names=[\"Left\", \"Center\", \"Right\"]):\n",
        "        \"\"\"\n",
        "        Evaluate the model and print classification report.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: Test dataset\n",
        "            target_names (list): Names of target classes\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing evaluation metrics\n",
        "        \"\"\"\n",
        "        if self.trainer is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Call train() first.\")\n",
        "\n",
        "        test_data = self.prepare_dataset(test_dataset)\n",
        "        predictions = self.trainer.predict(test_data)\n",
        "\n",
        "        # Get predictions and true labels\n",
        "        preds = predictions.predictions.argmax(-1)\n",
        "        labels = predictions.label_ids\n",
        "\n",
        "        # Generate report\n",
        "        report = classification_report(labels, preds, target_names=target_names, output_dict=True)\n",
        "        print(f\"ðŸ“Š DistilBERT Results:\")\n",
        "        print(classification_report(labels, preds, target_names=target_names))\n",
        "\n",
        "        return {\n",
        "            'preds': preds,\n",
        "            'labels': labels,\n",
        "            'probabilities': predictions.predictions,  # Raw logits, not probabilities\n",
        "            'confusion_matrix': confusion_matrix(labels, preds),\n",
        "            'report': report,\n",
        "            'predictions': predictions,  # Full prediction object\n",
        "            'accuracy': report['accuracy'],\n",
        "            'f1': report['macro avg']['f1-score'],\n",
        "            'precision': report['macro avg']['precision'],\n",
        "            'recall': report['macro avg']['recall']\n",
        "        }\n",
        "\n",
        "    def save_model(self, output_dir):\n",
        "        \"\"\"\n",
        "        Save the trained model and tokenizer.\n",
        "\n",
        "        Args:\n",
        "            output_dir (str): Directory to save model\n",
        "\n",
        "        Returns:\n",
        "            str: Path to saved model\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model to save. Train the model first.\")\n",
        "\n",
        "        self.model.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, model_dir, num_labels=3, max_length=512):\n",
        "        \"\"\"\n",
        "        Load a saved model and tokenizer.\n",
        "\n",
        "        Args:\n",
        "            model_dir (str): Directory containing the saved model\n",
        "            num_labels (int): Number of output labels\n",
        "            max_length (int): Maximum sequence length for tokenizer\n",
        "\n",
        "        Returns:\n",
        "            PoliticalBiasTransformer: Loaded model instance\n",
        "        \"\"\"\n",
        "        # Create instance\n",
        "        instance = cls(num_labels=num_labels, max_length=max_length)\n",
        "\n",
        "        # Load tokenizer\n",
        "        instance.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "        # Load model without class weights (they're not needed for inference)\n",
        "        instance.model = DistilBertForSequenceClassification.from_pretrained(model_dir)\n",
        "\n",
        "        return instance\n",
        "\n",
        "    def predict(self, texts):\n",
        "        \"\"\"\n",
        "        Make predictions on new texts.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text strings to classify\n",
        "\n",
        "        Returns:\n",
        "            tuple: (predictions, probabilities) where predictions are class indices\n",
        "                  and probabilities are softmax of logits\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model available. Train or load a model first.\")\n",
        "\n",
        "        # Tokenize texts\n",
        "        inputs = self.tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Move inputs to the same device as model\n",
        "        device = next(self.model.parameters()).device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Make predictions\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        # Get predicted classes and probabilities\n",
        "        logits = outputs.logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        return preds.cpu().numpy(), probs.cpu().numpy()\n",
        "\n",
        "    def convert_logits_to_probs(self, logits):\n",
        "        \"\"\"\n",
        "        Convert model logits to probabilities.\n",
        "\n",
        "        Args:\n",
        "            logits (array): Raw logits from model\n",
        "\n",
        "        Returns:\n",
        "            array: Probability distributions\n",
        "        \"\"\"\n",
        "        # Convert to PyTorch tensor if it's a numpy array\n",
        "        if isinstance(logits, np.ndarray):\n",
        "            logits_tensor = torch.tensor(logits)\n",
        "        else:\n",
        "            logits_tensor = logits\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probs = torch.nn.functional.softmax(logits_tensor, dim=-1)\n",
        "\n",
        "        # Convert back to numpy if input was numpy\n",
        "        if isinstance(logits, np.ndarray):\n",
        "            return probs.numpy()\n",
        "        else:\n",
        "            return probs"
      ]
    }
  ]
}